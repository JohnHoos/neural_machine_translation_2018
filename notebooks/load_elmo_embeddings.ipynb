{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import loader\n",
    "import rnn_encoder_decoder\n",
    "from torchtext import data\n",
    "import torch\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elmoformanylangs import Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 18:16:54,693 INFO: char embedding size: 15889\n",
      "2018-11-15 18:16:55,507 INFO: word embedding size: 140384\n",
      "2018-11-15 18:17:01,458 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(140384, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(15889, 50, padding_idx=15886)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "chinese_embedder = Embedder('/scratch/jp4989/elmo/179')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: \n",
    "# '/Users/yijialiu/work/projects/conll2018/models/word_elmo/cnn_50_100_512_4096_sample.json'\n",
    "vietnamese_embedder = Embedder('/scratch/jp4989/elmo/178')\n",
    "# FileNotFoundError: [Errno 2] No such file or directory: \n",
    "# '/Users/yijialiu/work/projects/conll2018/models/word_elmo/cnn_50_100_512_4096_sample.json'\n",
    "english_embedder = Embedder('/scratch/jp4989/elmo/144')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 19:08:36,182 INFO: char embedding size: 6169\n",
      "2018-11-15 19:08:36,583 INFO: word embedding size: 71222\n",
      "2018-11-15 19:08:40,127 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(71222, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(6169, 50, padding_idx=6166)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "simplified_chinese_embedder = Embedder('/scratch/jp4989/elmo/zhs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MUST USE GPU (?)\n",
    "parser = rnn_encoder_decoder.rnn_encoder_decoder_argparser()\n",
    "args = parser.parse_args([])\n",
    "#args.cpu = True\n",
    "args.split_chinese_into_characters = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common source vocabs: [('的', 272264), ('我', 83174), ('是', 71290), ('我们', 68696), ('在', 55727), ('了', 52197), ('你', 40824), ('这', 36049), ('一个', 33590), ('他们', 25256)]\n",
      "Source vocab size: 47127\n",
      "Most common english vocabs: [('the', 178698), ('and', 128020), ('to', 106119), ('of', 98919), ('a', 90534), ('that', 80454), ('i', 68877), ('in', 66951), ('it', 64198), ('you', 61161)]\n",
      "English vocab size: 28791\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, src, trg = loader.load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 19:12:41,640 INFO: 737 batches, avg len: 4.5\n",
      "2018-11-15 19:12:42,191 INFO: Finished 1000 sentences.\n",
      "2018-11-15 19:12:42,674 INFO: Finished 2000 sentences.\n",
      "2018-11-15 19:12:43,100 INFO: Finished 3000 sentences.\n",
      "2018-11-15 19:12:43,519 INFO: Finished 4000 sentences.\n",
      "2018-11-15 19:12:43,948 INFO: Finished 5000 sentences.\n",
      "2018-11-15 19:12:44,358 INFO: Finished 6000 sentences.\n",
      "2018-11-15 19:12:44,827 INFO: Finished 7000 sentences.\n",
      "2018-11-15 19:12:45,209 INFO: Finished 8000 sentences.\n",
      "2018-11-15 19:12:45,633 INFO: Finished 9000 sentences.\n",
      "2018-11-15 19:12:46,095 INFO: Finished 10000 sentences.\n",
      "2018-11-15 19:12:46,512 INFO: Finished 11000 sentences.\n",
      "2018-11-15 19:12:46,937 INFO: Finished 12000 sentences.\n",
      "2018-11-15 19:12:47,433 INFO: Finished 13000 sentences.\n",
      "2018-11-15 19:12:47,851 INFO: Finished 14000 sentences.\n",
      "2018-11-15 19:12:48,280 INFO: Finished 15000 sentences.\n",
      "2018-11-15 19:12:48,696 INFO: Finished 16000 sentences.\n",
      "2018-11-15 19:12:49,117 INFO: Finished 17000 sentences.\n",
      "2018-11-15 19:12:49,563 INFO: Finished 18000 sentences.\n",
      "2018-11-15 19:12:49,964 INFO: Finished 19000 sentences.\n",
      "2018-11-15 19:12:50,376 INFO: Finished 20000 sentences.\n",
      "2018-11-15 19:12:50,787 INFO: Finished 21000 sentences.\n",
      "2018-11-15 19:12:51,179 INFO: Finished 22000 sentences.\n",
      "2018-11-15 19:12:51,611 INFO: Finished 23000 sentences.\n",
      "2018-11-15 19:12:52,032 INFO: Finished 24000 sentences.\n",
      "2018-11-15 19:12:52,475 INFO: Finished 25000 sentences.\n",
      "2018-11-15 19:12:52,929 INFO: Finished 26000 sentences.\n",
      "2018-11-15 19:12:53,350 INFO: Finished 27000 sentences.\n",
      "2018-11-15 19:12:53,866 INFO: Finished 28000 sentences.\n",
      "2018-11-15 19:12:54,318 INFO: Finished 29000 sentences.\n",
      "2018-11-15 19:12:54,710 INFO: Finished 30000 sentences.\n",
      "2018-11-15 19:12:55,176 INFO: Finished 31000 sentences.\n",
      "2018-11-15 19:12:55,590 INFO: Finished 32000 sentences.\n",
      "2018-11-15 19:12:56,029 INFO: Finished 33000 sentences.\n",
      "2018-11-15 19:12:56,433 INFO: Finished 34000 sentences.\n",
      "2018-11-15 19:12:56,823 INFO: Finished 35000 sentences.\n",
      "2018-11-15 19:12:57,277 INFO: Finished 36000 sentences.\n",
      "2018-11-15 19:12:57,689 INFO: Finished 37000 sentences.\n",
      "2018-11-15 19:12:58,101 INFO: Finished 38000 sentences.\n",
      "2018-11-15 19:12:58,515 INFO: Finished 39000 sentences.\n",
      "2018-11-15 19:12:58,950 INFO: Finished 40000 sentences.\n",
      "2018-11-15 19:12:59,385 INFO: Finished 41000 sentences.\n",
      "2018-11-15 19:12:59,799 INFO: Finished 42000 sentences.\n",
      "2018-11-15 19:13:00,231 INFO: Finished 43000 sentences.\n",
      "2018-11-15 19:13:00,631 INFO: Finished 44000 sentences.\n",
      "2018-11-15 19:13:01,073 INFO: Finished 45000 sentences.\n",
      "2018-11-15 19:13:01,515 INFO: Finished 46000 sentences.\n",
      "2018-11-15 19:13:01,947 INFO: Finished 47000 sentences.\n"
     ]
    }
   ],
   "source": [
    "# WRONG WAY! this is embedding each character\n",
    "tokenized_itos = chinese_embedder.sents2elmo(src.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_itos[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG! unk eos should now be oov? and bos\n",
    "tokenized_itos_ver2 = chinese_embedder.sents2elmo([src.vocab.itos], output_layer=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlpclass]",
   "language": "python",
   "name": "conda-env-nlpclass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
